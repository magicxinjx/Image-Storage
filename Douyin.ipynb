{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. User Account Data Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to find and extract the encryption rule hidden in the JavaScript of the webpage:\n",
    "# JS → index_10ae3b3.js → _signature → _bytedAcrawler → \n",
    "# douyin_falcon:node_modules/byted-acrawler/dist/runtime → third_54fd252.js → \n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "#import asyncio\n",
    "#import aiohttp\n",
    "import datetime\n",
    "import requests\n",
    "import bs4 as bs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from time import sleep\n",
    "\n",
    "headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \\\n",
    "(KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'}\n",
    "video_headers ={'authority': 'aweme.snssdk.com','method': 'GET','scheme': 'https',\n",
    "'user-agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) \\\n",
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Mobile Safari/537.36',\n",
    "'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,\\\n",
    "image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "'accept-encoding': 'gzip, deflate, br',\n",
    "'accept-language': 'en-US,en;q=0.9',\n",
    "'upgrade-insecure-requests': '1',\n",
    "'cache-control': 'max-age=0',\n",
    "'sec-fetch-mode': 'navigate',\n",
    "'sec-fetch-site': 'none',\n",
    "'sec-fetch-user': '?1'}\n",
    "\n",
    "def proxy():\n",
    "    proxy = requests.get('http://api.ip.data5u.com/dynamic/\\\n",
    "    get.html?order=8ed0b4d65dffaa785c0ccc76b5929c59&sep=3').text.strip()\n",
    "    print('the current ip: '+proxy.split(':')[0])\n",
    "    return {'http':proxy,'https':proxy}\n",
    "def ip():\n",
    "    re = requests.get('http://www.882667.com/',proxies = proxy())\n",
    "    soup = bs.BeautifulSoup(re.content,'html.parser')\n",
    "    return 'current ip: '+[i.text for i in soup.find_all('p')\\\n",
    "                           if '你的ip是' in str(i)][0].split('\\u3000')[0].split('：')[1]\n",
    "def generate_path(path):\n",
    "    folder = os.path.exists(path)\n",
    "    if not folder:\n",
    "        os.makedirs(path)\n",
    "def video(video_url,file_name):\n",
    "    return urllib.request.urlretrieve(requests.get(video_url,headers = video_headers).url,file_name)\n",
    "def cover(cover_url,file_name):\n",
    "    return urllib.request.urlretrieve(cover_url,file_name)\n",
    "def real_time(timestamp):\n",
    "    return str(datetime.datetime.fromtimestamp(timestamp))\n",
    "def param(share_url):\n",
    "    re = requests.get(share_url,headers=headers)\n",
    "    #uid=re.text.split('uid: ')[1][1:12]\n",
    "    dytk=re.text.split('dytk: ')[1][1:33]\n",
    "    nickname=re.text.split('nickname\">')[1].split('</p>')[0]\n",
    "    if '&u_code=' in re.url:\n",
    "        sec_uid=re.url.split('sec_uid=')[1].split('&u_code=')[0]\n",
    "    else:\n",
    "        sec_uid=re.url.split('sec_uid=')[1].split('&utm_')[0] \n",
    "    return [sec_uid,dytk,nickname]\n",
    "def get_api_(sec_uid,max_cursor,dytk):\n",
    "    API=[]\n",
    "    def page(sec_uid,max_cursor,dytk):\n",
    "        url='https://www.amemv.com/web/api/v2/aweme/post/?'\n",
    "        params={'sec_uid':sec_uid,'count':'21','max_cursor':max_cursor,'aid':'1128','dytk':dytk}\n",
    "        re=requests.get(url,headers = headers, params=params)\n",
    "        #print(re.url,'\\n')\n",
    "        API.append(re.url)\n",
    "        data = re.json()\n",
    "        if data['has_more'] == True:\n",
    "            return page(sec_uid, data['max_cursor'], dytk)\n",
    "            time.sleep(random.uniform(6,8))\n",
    "        else:\n",
    "            return API\n",
    "    return page(sec_uid,max_cursor,dytk)\n",
    "def get_api(share_url):\n",
    "    p=param(share_url)\n",
    "    get_api_(p[0],'0',p[1])\n",
    "    return get_api_(p[0],'0',p[1])\n",
    "def douyin_account(share_url):\n",
    "    API=get_api(share_url)\n",
    "    num_api = 0\n",
    "    for api in API:\n",
    "        print(f'API {num_api},'+' '+api,'\\n')\n",
    "        num_api=num_api+1\n",
    "        re = requests.get(api).json()\n",
    "        data = re['aweme_list']\n",
    "        desc = [info['desc'] for info in data]\n",
    "        nickname = [info['author']['nickname'] for info in data]\n",
    "        aweme_id = [info['statistics']['aweme_id'] for info in data]\n",
    "        share_count = [info['statistics']['share_count'] for info in data]\n",
    "        forward_count = [info['statistics']['forward_count'] for info in data]\n",
    "        like_count = [info['statistics']['digg_count'] for info in data]\n",
    "        comment_count = [info['statistics']['comment_count'] for info in data]\n",
    "        cover_url = [info['video']['cover']['url_list'][0] for info in data]\n",
    "        cover_visual = ['<img src=\"'+ url + '\" width=\"100\" >' for url in cover_url]\n",
    "        aweme_id = [str(info['statistics']['aweme_id'])+'\\t' for info in data]\n",
    "\n",
    "        hashtag_name=[]\n",
    "        for info in re['aweme_list']:\n",
    "            hashtag_name_=[]\n",
    "            for info_ in info['text_extra']:\n",
    "                try:\n",
    "                    h=str(info_['hashtag_name'])\n",
    "                except:\n",
    "                    h=str('')\n",
    "                if len(h)>0 and h!='0':\n",
    "                    hashtag_name_.append(h)\n",
    "            hashtag_name.append(', '.join(hashtag_name_))\n",
    "    \n",
    "        hashtag_id=[]\n",
    "        for info in re['aweme_list']:\n",
    "            hashtag_id_=[]\n",
    "            for info_ in info['text_extra']:\n",
    "                try:\n",
    "                    h=str(info_['hashtag_id'])\n",
    "                except:\n",
    "                    h=str('')\n",
    "                if len(h)>0 and h!='0':\n",
    "                    hashtag_id_.append(h+'\\t')\n",
    "            hashtag_id.append(', '.join(hashtag_id_))\n",
    "            \n",
    "        height = [info['video']['height'] for info in data]\n",
    "        width = [info['video']['width'] for info in data]\n",
    "        ratio = [info['video']['ratio'] for info in data]\n",
    "        duration = [round(info['video']['duration']/1000) for info in data]\n",
    "        video_url = []\n",
    "        for info in data:\n",
    "            video_url.append([i for i in info['video']['download_addr']['url_list'] if 'aweme' in i][0].\\\n",
    "            replace('watermark=1','watermark=0'))\n",
    "        df=pd.DataFrame({'desc':desc,'nickname':nickname,'share_count':share_count,'forward_count':forward_count,\n",
    "                         'like_count':like_count,'comment_count':comment_count,'video_url':video_url,\n",
    "                         'cover_visual':cover_visual, 'video_id': aweme_id,'ratio':ratio,'duration':duration,\n",
    "                         'hashtag_name':hashtag_name,'hashtag_id':hashtag_id,'height':height,'width':width})\n",
    "        df['period_from'] = real_time(int(str(re['max_cursor'])[:-3]))\n",
    "        df['period_to'] = real_time(int(str(re['min_cursor'])[:-3]))\n",
    "        columns = ['nickname','desc','hashtag_name','hashtag_id','video_id','share_count','forward_count',\n",
    "                   'like_count','comment_count','video_url','cover_visual','height','width','ratio',\n",
    "                   'duration','period_from','period_to']\n",
    "        \n",
    "        file = os.path.exists('./videos_by_account.csv')\n",
    "        if not file:\n",
    "            df.to_csv('./videos_by_account.csv',encoding = 'utf-8-sig', index = False, columns = columns,\n",
    "                     float_format='%f')\n",
    "        else:\n",
    "            df.to_csv('./videos_by_account.csv',encoding = 'utf-8-sig', index = False, columns = columns,\n",
    "                     float_format='%f', mode='a', header = False)\n",
    "            \n",
    "        # video downloading\n",
    "        generate_path('./videos_by_account')\n",
    "        for num in range(0,len(df)):\n",
    "            try:\n",
    "                video(df['video_url'][num],'./videos_by_account/'+\n",
    "                      df['nickname'][num]+'-'+df['video_id'][num].replace('\\t','')+'.mp4')\n",
    "                print('Account: '+df['nickname'][num]+', Video: '+df['video_id'][num]+'......Succeeded')\n",
    "            except:\n",
    "                print('Account: '+df['nickname'][num]+', Video: '+df['video_id'][num]+'......Failed')\n",
    "            time.sleep(random.uniform(4,5))\n",
    "        print('\\n')\n",
    "\n",
    "url_csv = pd.read_excel('ball.xlsx',encoding = 'utf-8-sig')\n",
    "task_num = 0\n",
    "for share_url in url_csv['Link'][6:]:\n",
    "    print('####################################################################')\n",
    "    print(f'Task: {task_num}, '+'Account: '+param(share_url)[2]+', Share Url: '+share_url,'\\n')\n",
    "    douyin_account(share_url)\n",
    "    task_num = task_num + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hot Trending Data Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import requests\n",
    "import bs4 as bs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from IPython.core.display import HTML\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "video_headers ={'authority': 'aweme.snssdk.com','method': 'GET','scheme': 'https',\n",
    "'user-agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) \\\n",
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Mobile Safari/537.36',\n",
    "'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,\\\n",
    "image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "'accept-encoding': 'gzip, deflate, br',\n",
    "'accept-language': 'en-US,en;q=0.9',\n",
    "'upgrade-insecure-requests': '1',\n",
    "'cache-control': 'max-age=0',\n",
    "'sec-fetch-mode': 'navigate',\n",
    "'sec-fetch-site': 'none',\n",
    "'sec-fetch-user': '?1'}\n",
    "\n",
    "def proxy():\n",
    "    proxy = requests.get('http://api.ip.data5u.com/dynamic/get.html?order=8ed0b4d65dffaa785c0ccc76b5929c59&sep=3').text.strip()\n",
    "    print('the current ip: '+proxy.split(':')[0])\n",
    "    return {'http':proxy,'https':proxy}\n",
    "def ip():\n",
    "    re = requests.get('http://www.882667.com/',proxies = proxy())\n",
    "    soup = bs.BeautifulSoup(re.content,'html.parser')\n",
    "    return 'current ip: '+[i.text for i in soup.find_all('p') if '你的ip是' in str(i)][0].split('\\u3000')[0].split('：')[1]\n",
    "def generate_path(path):\n",
    "    folder = os.path.exists(path)\n",
    "    if not folder:\n",
    "        os.makedirs(path)\n",
    "def video(video_url,file_name):\n",
    "    return urllib.request.urlretrieve(requests.get(video_url,headers=video_headers).url,file_name)\n",
    "def cover(cover_url,file_name):\n",
    "    return urllib.request.urlretrieve(cover_url,file_name)\n",
    "def real_time(timestamp):\n",
    "    return str(datetime.datetime.fromtimestamp(timestamp))\n",
    "\n",
    "api = 'https://aweme-hl.snssdk.com/aweme/v1/hot/search/list/?detail_list=1&mac_address=\\\n",
    "08:00:27:29:D2:F5&os_api=23&device_type=MI%205s&device_platform=android&ssmix=a&iid=\\\n",
    "92152480453&manifest_version_code=860&dpi=320&uuid=008796750074613&version_code=\\\n",
    "860&app_name=aweme&version_name=8.6.0&ts=1577932778&openudid=c055533a0591b2dc&device_id=\\\n",
    "69918538596&resolution=810*1440&os_version=6.0.1&language=zh&device_brand=Xiaomi&app_type=\\\n",
    "normal&ac=wifi&update_version_code=8602&aid=1128&channel=tengxun_new&_rticket=1577932779592'\n",
    "re=requests.get(api)\n",
    "#print('the trend_ip: '+trend_proxy.split(':')[0])\n",
    "last_update = re.json()['data']['active_time']\n",
    "trending_data = re.json()['data']['word_list']\n",
    "trend = pd.DataFrame(trending_data)\n",
    "trend['group_id']=trend['group_id'].astype(str)\n",
    "Word_cover=[]\n",
    "for i in trend['word_cover']:\n",
    "    if type(i)==dict:\n",
    "        Word_cover.append(i['url_list'][0])\n",
    "    else:\n",
    "        Word_cover.append(None)\n",
    "\n",
    "date=last_update.split(' ')[0]\n",
    "time=last_update.split(' ')[1]\n",
    "trend = trend.drop(columns = ['word_cover','challenge_id'])\n",
    "trend['collection_date'] = date\n",
    "trend['collection_time'] = time\n",
    "if 'ad_data' in trend.columns:\n",
    "    trend = trend.drop(columns = ['ad_data','search_word'])\n",
    "file = os.path.exists('./trend.csv')\n",
    "columns = ['position','word','video_count','hot_value','collection_date',\n",
    "           'collection_time','word_type','label','group_id']\n",
    "if not file:\n",
    "    trend.to_csv('./trend.csv',encoding = 'utf-8-sig', index = False, columns = columns)\n",
    "else:\n",
    "    trend.to_csv('./trend.csv',encoding = 'utf-8-sig', index = False, mode='a', columns = columns, header=False)\n",
    "\n",
    "# core function\n",
    "def scraper(topic):\n",
    "    # data collecting\n",
    "    topic_api='https://aweme-hl.snssdk.com/aweme/v1/hot/search/video/list/?hotword='\n",
    "    re=requests.get(topic_api+topic+'&count=2000').json()\n",
    "    print(topic_api+topic+'&count=2000')\n",
    "    data = re['aweme_list']\n",
    "    desc = [info['desc'] for info in data]\n",
    "    time_stamp = [info['create_time'] for info in data]\n",
    "    create_time = [real_time(info['create_time']) for info in data]\n",
    "    nickname = [info['author']['nickname'] for info in data]\n",
    "    verify = [info['author']['custom_verify'] for info in data]\n",
    "    share_count = [info['statistics']['share_count'] for info in data]\n",
    "    forward_count = [info['statistics']['forward_count'] for info in data]\n",
    "    like_count = [info['statistics']['digg_count'] for info in data]\n",
    "    comment_count = [info['statistics']['comment_count'] for info in data]\n",
    "    download_count = [info['statistics']['download_count'] for info in data]\n",
    "    cover_url = [info['video']['cover']['url_list'][0] for info in data]\n",
    "    cover_visual = ['<img src=\"'+ url + '\" width=\"100\" >' for url in cover_url]\n",
    "    aweme_id = [info['statistics']['aweme_id']+'\\t' for info in data]\n",
    "    enterprise_verify_reason = [info['author']['enterprise_verify_reason'] for info in data]\n",
    "    height = [info['video']['height'] for info in data]\n",
    "    width = [info['video']['width'] for info in data]\n",
    "    ratio = [info['video']['ratio'] for info in data]\n",
    "    duration = [round(info['video']['duration']/1000) for info in data]  \n",
    "\n",
    "    hashtag_name=[]\n",
    "    for info in re['aweme_list']:\n",
    "        hashtag_name_=[]\n",
    "        for info_ in info['text_extra']:\n",
    "            try:\n",
    "                h=str(info_['hashtag_name'])\n",
    "            except:\n",
    "                h=str('')\n",
    "            if len(h)>0 and h!='0':\n",
    "                hashtag_name_.append(h)\n",
    "        hashtag_name.append(', '.join(hashtag_name_))\n",
    "    \n",
    "    hashtag_id=[]\n",
    "    for info in re['aweme_list']:\n",
    "        hashtag_id_=[]\n",
    "        for info_ in info['text_extra']:\n",
    "            try:\n",
    "                h=str(info_['hashtag_id'])\n",
    "            except:\n",
    "                h=str('')\n",
    "            if len(h)>0 and h!='0':\n",
    "                hashtag_id_.append(h+'\\t')\n",
    "        hashtag_id.append(', '.join(hashtag_id_))\n",
    "            \n",
    "          \n",
    "    video_url = []\n",
    "    for info in data:\n",
    "        video_url.append([i for i in info['video']['download_addr']['url_list'] if 'aweme-hl' in i][0].\\\n",
    "                         replace('watermark=1','watermark=0'))\n",
    "        \n",
    "    df=pd.DataFrame({'topic_name':topic, 'desc':desc,'hashtag_name':hashtag_name,'hashtag_id':hashtag_id,\n",
    "                     'nickname':nickname,'create_time':create_time,'verify':verify,'time_stamp':time_stamp,\n",
    "                     'share_count':share_count,'forward_count':forward_count,'like_count':like_count,\n",
    "                     'comment_count':comment_count,'download_count':download_count,'video_url':video_url,\n",
    "                     'cover_visual':cover_visual,'video_id': aweme_id,'height':height,'width':width,\n",
    "                     'ratio':ratio,'duration':duration,'enterprise_verify_reason':enterprise_verify_reason})\n",
    "    \n",
    "    \n",
    "    columns = ['topic_name','nickname','desc','hashtag_name','hashtag_id','video_id','create_time','duration',\n",
    "               'share_count','forward_count','like_count','comment_count','download_count','verify',\n",
    "               'enterprise_verify_reason','time_stamp','video_url','cover_visual','height','width','ratio']\n",
    "    \n",
    "    file = os.path.exists('./videos.csv')\n",
    "    if not file:\n",
    "        df.to_csv('./videos.csv',encoding = 'utf-8-sig', index = False, columns = columns,float_format='{:f}'.format)\n",
    "    else:\n",
    "        df.to_csv('./videos.csv',encoding = 'utf-8-sig', index = False, mode='a',\n",
    "                                 header = False, columns = columns,float_format='{:f}'.format)\n",
    "        \n",
    "    # video downloading\n",
    "    generate_path('./videos')\n",
    "    index=0\n",
    "    for num in range(0,len(data)):\n",
    "        try:\n",
    "            video(df['video_url'][num],'./videos/'+topic+'-'+df['video_id'][num].replace('\\t','')+'.mp4')\n",
    "            print(topic+', video #'+str(df['video_id'][num])+'......Successed')\n",
    "        except:\n",
    "            print(topic+', video #'+str(df['video_id'][num])+'......Failed')\n",
    "            continue\n",
    "def douyin_trend():\n",
    "    for topic_ in trend['word']:\n",
    "        scraper(topic_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
